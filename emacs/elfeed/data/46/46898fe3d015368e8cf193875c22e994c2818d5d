<p>
Everybody seems to be very excited about generative AI models these days. The availability of the Large Language Models (LLMs) through conversational interfaces like ChatGPT and of image generation tools like Dall-E or Stable Diffusion have brought generative AI to the masses. Although Github Copilot, also based on the GPT family of models, has been available for a while now, this was a niche tool, for programmers only. 
</p>

<p>
Of course, Emacs being the best text-based interface to a computer, it is also the best interface to the generative AI models which are driven through textual prompts. Everything being a text buffer in Emacs, sending and receiving text via an API is straightforward (if you know Emacs Lisp).
</p>

<p>
It is therefore not a surprise that there are many Emacs packages allowing to use ChatGPT and Copilot. Just to list a few<sup><a class="footref" href="https://dindi.garjola.net/rss.xml#fn.1" id="fnr.1">1</a></sup>:
</p>

<ul class="org-ul">
<li><a href="https://github.com/antonhibl/gptai">gptai</a></li>
<li><a href="https://github.com/karthink/gptel">gptel</a></li>
<li><a href="https://github.com/rksm/org-ai">org-ai</a></li>
<li><a href="https://github.com/xenodium/chatgpt-shell">chatgpt-shell</a></li>
<li><a href="https://github.com/zerolfx/copilot.el">copilot.el</a></li>
</ul>

<p>
Recently, David Wilson at System Crafters did <a href="https://www.youtube.com/watch?v=JImYEdqVQR8">a live stream</a> showing some of the capabilities of these packages. To be honest, I was not impressed by the results of the code generated by the LLMs, but I can understand that many people may find them useful.
</p>

<p>
At the end of the stream, David wanted to address the question of the problems and issues with using these models. Of course, being a programmer, David likes recursion, and he asked ChatGPT about that. The LLM answered, as usual, with a balanced view with pros and cons. It is also usual for these models, in my opinion, to give awfully banal answers. There can be legal issues, copyright ones (in both senses, that is, that the models are trained with copyrighted material, and that the copyright of their outputs is not well defined), ethical problems, etc.
</p>

<p>
As always, the Silicon Valley tech firms have not waited for these issues to be settled before deploying the technology to the public. They impose their vision regardless of the consequences. Unfortunately, most programmers like the tech so much that they don't stop thinking before adopting and participating in spreading it.
</p>

<p>
Emacs being one of the most important contributions of the Free Software community, it may be surprising that some of its users are so keen to ride the <a href="https://www.jordiinglada.net/sblog/llm.html">latest Trojan horse of technofeudalism</a>. Fermin pointed out a <a href="https://sasanidas.gitlab.io/f-site/lsp-troyan/">similar kind of issue with the Language Server Protocol</a>, but that was not a real problem, since LSP servers run locally and we have free software implementations of them. This is not the case for ChatGPT or Copilot. We only have an API that can be taken down at any moment. But worse than that, as <a href="https://www.jordiinglada.net/sblog/llm.html">pointed out</a>, using these LLMs means that we are working in training them. Every time that David, in his demo, wrote <i>"â€¦ the code is wrong because of â€¦, can you fix it?"</i>, he was giving feedback for the reinforcement learning of ChatGPT.
</p>

<p>
So what can we do? Stop using these tools and loose our programming or writing jobs because we will be less productive than those that use them?
</p>

<p>
Maybe we can do what GNU hackers and other Free Software activists have always done: implement libre tools that are digital common goods that free and empower users. Building and training big AI models is very costly and may be much more difficult than building an OS kernel like Linux, GCC or Emacs, but fortunately, there are already <a href="https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76">alternatives to ChatGPT</a>.
</p>

<p>
The best starting point could be helping the <a href="https://bigscience.huggingface.co/">BigScience Project</a> by using their <a href="https://huggingface.co/bigscience/bloom">Bloom</a> model from Emacs to train it and improve it. Hints on how to install Bloom locally and use it with Python can be found <a href="https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32">in this blog post</a>. There are also initiatives to use <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a><sup><a class="footref" href="https://dindi.garjola.net/rss.xml#fn.2" id="fnr.2">2</a></sup> to <a href="https://www.reddit.com/r/MachineLearning/comments/zl03b0/project_run_and_finetune_bloom176b_at_home_using/">improve Bloom</a>. The <a href="https://www.bigcode-project.org/">BigCode</a> project targets code generation, and is to BigScience what Copilot is to ChatGPT. You can play with it <a href="https://huggingface.co/chat">here</a>. There is a <a href="https://marketplace.visualstudio.com/items?itemName=Lisoveliy.starcoderex">VSCode plugin</a> for their StarCoder model, but no Emacs package yet. Isn't that a shame?
</p>

<p>
BigScience is Open Science, while OpenAI's <b>is not open</b>, but proprietary technology built from common digital goods harvested on the internet. It shouldnâ€™t be difficult for us, Emacs users, to choose who we want to help.
</p>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a class="footnum" href="https://dindi.garjola.net/rss.xml#fnr.1" id="fn.1">1</a></sup> <div class="footpara"><p class="footpara">
In alphabetical order using <code>M-x sort-lines</code> ðŸ˜‰
</p></div></div>

<div class="footdef"><sup><a class="footnum" href="https://dindi.garjola.net/rss.xml#fnr.2" id="fn.2">2</a></sup> <div class="footpara"><p class="footpara">
Yes, federated like in <a href="https://en.wikipedia.org/wiki/Fediverse">Fediverse</a>!
</p></div></div>


</div>
</div><div class="taglist"><a href="https://dindi.garjola.net/tags.html">Tags</a>: <a href="https://dindi.garjola.net/tag-emacs.html">emacs</a> <a href="https://dindi.garjola.net/tag-ai.html">ai</a> <a href="https://dindi.garjola.net/tag-chatgpt.html">chatgpt</a> <a href="https://dindi.garjola.net/tag-open-science.html">open-science</a> </div>